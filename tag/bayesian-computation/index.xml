<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian Computation | Academic</title>
    <link>https://maurilorenzo/maurilorenzo.github.io/tag/bayesian-computation/</link>
      <atom:link href="https://maurilorenzo/maurilorenzo.github.io/tag/bayesian-computation/index.xml" rel="self" type="application/rss+xml" />
    <description>Bayesian Computation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 30 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://maurilorenzo/maurilorenzo.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Bayesian Computation</title>
      <link>https://maurilorenzo/maurilorenzo.github.io/tag/bayesian-computation/</link>
    </image>
    
    <item>
      <title>The Stochastic Barker Proposal</title>
      <link>https://maurilorenzo/maurilorenzo.github.io/project/sbp/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://maurilorenzo/maurilorenzo.github.io/project/sbp/</guid>
      <description>&lt;p&gt;Markov chain Monte Carlo (MCMC) algorithms represent the most common method for solving intractable integrals in Bayesian inference. Among this class of algorithms, stochastic gradient MCMC (SGMCMC) have recently gained great attention as they combine scalability to large datasets with a fast exploration of the space. 
In this thesis, we study the Stochastic Barker proposal, the stochastic gradient version of a recently introduced MCMC algorithm, the Barker scheme that has been shown to outperform standard gradient based MCMC in terms of robustness to hyperparameter tuning. The method presented is compared to the stochastic gradient Langevin dynamics (SGLD) algorithm, a popular SGMCMC. Also in the stochastic case, the Barker Proposal shows greater robustness to hyperparameter tuning when the posterior is irregular and outperforms SGLD in terms of predictive accuracy on real datasets.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
