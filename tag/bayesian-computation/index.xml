<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian Computation | Lorenzo Mauri</title>
    <link>https://maurilorenzo.github.io/tag/bayesian-computation/</link>
      <atom:link href="https://maurilorenzo.github.io/tag/bayesian-computation/index.xml" rel="self" type="application/rss+xml" />
    <description>Bayesian Computation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 30 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://maurilorenzo.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Bayesian Computation</title>
      <link>https://maurilorenzo.github.io/tag/bayesian-computation/</link>
    </image>
    
    <item>
      <title>The Stochastic Barker Proposal</title>
      <link>https://maurilorenzo.github.io/project/sbp/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://maurilorenzo.github.io/project/sbp/</guid>
      <description>&lt;p&gt;Stochastic Gradient (SG) Markov Chain Monte Carlo algorithms (MCMC) are popular algorithms for Bayesian sampling in the presence of large datasets. 
However, they come with little theoretical guarantees and assessing their empirical performances is non-trivial.
In such context, it is crucial to develop algorithms that are robust to the choice of hyperparameters and to gradients heterogeneity since, in practice, 
both the choice of step-size and behaviour of target gradients induce hard-to-control  biases in the invariant distribution. 
In this work we introduce the stochastic gradient Barker dynamics (SGBD) algorithm, 
extending the recently developed Barker MCMC scheme, a robust alternative to Langevin-based sampling algorithms, to the stochastic gradient framework.
We characterize the impact of stochastic gradients on the Barker transition mechanism and develop a bias-corrected version 
that, under suitable assumptions, eliminates the error due to the gradient noise in the proposal.
We illustrate the performance on a number of high-dimensional examples, showing that SGBD is more robust to hyperparameter tuning and to irregular behavior of the target gradients compared to the popular stochastic gradient Langevin dynamics algorithm.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
